{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"randomcrop_dataaug_ppo_bigfish.ipynb","provenance":[{"file_id":"1JZFwKVQryXdr_l13vsYhrRz8DQx0yP9L","timestamp":1606939931806},{"file_id":"1FPtv9ViOvIp7S5qyDXw1ilRYTgwPwmnY","timestamp":1606736144721},{"file_id":"1Oxz-yvTTmZHiDoPZNpxPOGBlHglllzvs","timestamp":1606295085832},{"file_id":"1hAOVglijseKkHru85r9-RoLl_fZo1sIJ","timestamp":1605725392941},{"file_id":"1LNknRcS0vP5ocaH9fRfV4uvpFgsq6CBo","timestamp":1605540061171},{"file_id":"19eg-qXLO6ywbDl66FJ9AQl46Wjvs1qja","timestamp":1604661124100},{"file_id":"1d4iEmvk2z6v0-M4LtnjIeuXSJPadZ7M8","timestamp":1604306820961},{"file_id":"11nnS8AAIxo5HYcXaSWLo8eGnrePeYOpO","timestamp":1604049801984}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K1_WKdcrI6w3"},"source":["# Getting started with PPO and ProcGen"]},{"cell_type":"markdown","metadata":{"id":"z7LP1JU3I-d4"},"source":["Here's a bit of code that should help you get started on your projects.\n","\n","The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdpZ4lmFHtD8","executionInfo":{"status":"ok","timestamp":1607551343902,"user_tz":-60,"elapsed":14286,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}},"outputId":"9b851a43-d191-45d2-ed1d-001ff548d329"},"source":["!pip install procgen\n","!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n","!wget https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting procgen\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/34/0ae32b01ec623cd822752e567962cfa16ae9c6d6ba2208f3445c017a121b/procgen-0.10.4-cp36-cp36m-manylinux2010_x86_64.whl (39.9MB)\n","\u001b[K     |████████████████████████████████| 39.9MB 83kB/s \n","\u001b[?25hRequirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.17.3)\n","Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (3.0.12)\n","Collecting gym3<1.0.0,>=0.3.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/8c/83da801207f50acfd262041e7974f3b42a0e5edd410149d8a70fd4ad2e70/gym3-0.3.3-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (1.18.5)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n","Collecting imageio-ffmpeg<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/12/01126a2fb737b23461d7dadad3b8abd51ad6210f979ff05c6fa9812dfbbe/imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl (22.2MB)\n","\u001b[K     |████████████████████████████████| 22.2MB 1.3MB/s \n","\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.14.3)\n","Collecting glfw<2.0.0,>=1.8.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/1b/cc758368f1b2466b3701c0f692973aa8a0b51a192a40463c1d02d54d640c/glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203kB)\n","\u001b[K     |████████████████████████████████| 204kB 34.9MB/s \n","\u001b[?25hCollecting imageio<3.0.0,>=2.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 55.9MB/s \n","\u001b[?25hCollecting moderngl<6.0.0,>=5.5.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/ab/5f72a1b7c5bdbb17160c85e8ba855d48925c74ff93c1e1027d5ad40bf33c/moderngl-5.6.2-cp36-cp36m-manylinux1_x86_64.whl (664kB)\n","\u001b[K     |████████████████████████████████| 665kB 51.5MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.20)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.0.0)\n","Collecting glcontext<3,>=2\n","  Downloading https://files.pythonhosted.org/packages/b0/8d/93915df9cd8d31c5f054bbacd1c7a76cd2f776b8212dcc768358bd2d4a37/glcontext-2.2.0-cp36-cp36m-manylinux1_x86_64.whl\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: imageio-ffmpeg, glfw, imageio, glcontext, moderngl, gym3, procgen\n","  Found existing installation: imageio 2.4.1\n","    Uninstalling imageio-2.4.1:\n","      Successfully uninstalled imageio-2.4.1\n","Successfully installed glcontext-2.2.0 glfw-1.12.0 gym3-0.3.3 imageio-2.9.0 imageio-ffmpeg-0.3.0 moderngl-5.6.2 procgen-0.10.4\n","--2020-12-09 22:02:23--  https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14807 (14K) [text/plain]\n","Saving to: ‘utils.py’\n","\n","utils.py            100%[===================>]  14.46K  --.-KB/s    in 0s      \n","\n","2020-12-09 22:02:23 (30.0 MB/s) - ‘utils.py’ saved [14807/14807]\n","\n","--2020-12-09 22:02:23--  https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7657 (7.5K) [text/plain]\n","Saving to: ‘TransformLayer.py’\n","\n","TransformLayer.py   100%[===================>]   7.48K  --.-KB/s    in 0s      \n","\n","2020-12-09 22:02:24 (78.4 MB/s) - ‘TransformLayer.py’ saved [7657/7657]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yDfs7DVPvlhp"},"source":["Data aug code from\n","https://github.com/MishaLaskin/rad/blob/1246bfd6e716669126e12c1f02f393801e1692c1/data_augs.py#L296\n"]},{"cell_type":"code","metadata":{"id":"rhVaMUy7ukKz","executionInfo":{"status":"ok","timestamp":1607551347888,"user_tz":-60,"elapsed":18037,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":["'''\n","dataaugs:\n","https://github.com/MishaLaskin/rad/blob/1246bfd6e716669126e12c1f02f393801e1692c1/data_augs.py#L296\n","'''\n","'''\n","https://arxiv.org/pdf/2004.14990.pdf\n","'''\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from TransformLayer import ColorJitterLayer\n","\n","\n","def random_crop(imgs, out=84):\n","    \"\"\"\n","        args:\n","        imgs: np.array shape (B,C,H,W)\n","        out: output size (e.g. 84)\n","        returns np.array\n","    \"\"\"\n","    n, c, h, w = imgs.shape\n","    crop_max = h - out + 1\n","    w1 = np.random.randint(0, crop_max, n)\n","    h1 = np.random.randint(0, crop_max, n)\n","    cropped = np.empty((n, c, out, out), dtype=imgs.dtype)\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        \n","        cropped[i] = img[:, h11:h11 + out, w11:w11 + out]\n","    return cropped\n","\n","\n","def grayscale(imgs):\n","    # imgs: b x c x h x w\n","    device = imgs.device\n","    b, c, h, w = imgs.shape\n","    frames = c // 3\n","    \n","    imgs = imgs.view([b,frames,3,h,w])\n","    imgs = imgs[:, :, 0, ...] * 0.2989 + imgs[:, :, 1, ...] * 0.587 + imgs[:, :, 2, ...] * 0.114 \n","    \n","    imgs = imgs.type(torch.uint8).float()\n","    # assert len(imgs.shape) == 3, imgs.shape\n","    imgs = imgs[:, :, None, :, :]\n","    imgs = imgs * torch.ones([1, 1, 3, 1, 1], dtype=imgs.dtype).float().to(device) # broadcast tiling\n","    return imgs\n","\n","def random_grayscale(images,p=.3):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: cpu or cuda\n","        returns torch.tensor\n","    \"\"\"\n","    device = images.device\n","    in_type = images.type()\n","    images = images * 255.\n","    images = images.type(torch.uint8)\n","    # images: [B, C, H, W]\n","    bs, channels, h, w = images.shape\n","    images = images.to(device)\n","    gray_images = grayscale(images)\n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = torch.from_numpy(mask)\n","    frames = images.shape[1] // 3\n","    images = images.view(*gray_images.shape)\n","    mask = mask[:, None] * torch.ones([1, frames]).type(mask.dtype)\n","    mask = mask.type(images.dtype).to(device)\n","    mask = mask[:, :, None, None, None]\n","    out = mask * gray_images + (1 - mask) * images\n","    out = out.view([bs, -1, h, w]).type(in_type) / 255.\n","    return out\n","\n","# random cutout\n","# TODO: should mask this \n","\n","def random_cutout(imgs, min_cut=10,max_cut=30):\n","    \"\"\"\n","        args:\n","        imgs: np.array shape (B,C,H,W)\n","        min / max cut: int, min / max size of cutout \n","        returns np.array\n","    \"\"\"\n","\n","    n, c, h, w = imgs.shape\n","    w1 = np.random.randint(min_cut, max_cut, n)\n","    h1 = np.random.randint(min_cut, max_cut, n)\n","    \n","    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        cut_img = img.copy()\n","        cut_img[:, h11:h11 + h11, w11:w11 + w11] = 0\n","        #print(img[:, h11:h11 + h11, w11:w11 + w11].shape)\n","        cutouts[i] = cut_img\n","    return cutouts\n","\n","def random_cutout_color(imgs, min_cut=7,max_cut=22):\n","    \"\"\"\n","        args:\n","        imgs: shape (B,C,H,W)\n","        out: output size (e.g. 84)\n","    \"\"\"\n","    \n","    n, c, h, w = imgs.shape\n","    w1 = np.random.randint(min_cut, max_cut, n)\n","    h1 = np.random.randint(min_cut, max_cut, n)\n","    \n","    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n","    rand_box = np.random.randint(0, 255, size=(n, c)) / 255.\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        cut_img = img.copy()\n","        \n","        # add random box\n","        cut_img[:, h11:h11 + h11, w11:w11 + w11] = np.tile(\n","            rand_box[i].reshape(-1,1,1),                                                \n","            (1,) + cut_img[:, h11:h11 + h11, w11:w11 + w11].shape[1:])\n","        \n","        cutouts[i] = cut_img\n","    return cutouts\n","\n","# random flip\n","\n","def random_flip(images,p=.2):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: cpu or gpu, \n","        p: prob of applying aug,\n","        returns torch.tensor\n","    \"\"\"\n","    # images: [B, C, H, W]\n","    device = images.device\n","    bs, channels, h, w = images.shape\n","    \n","    images = images.to(device)\n","\n","    flipped_images = images.flip([3])\n","    \n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = torch.from_numpy(mask)\n","    frames = images.shape[1] #// 3\n","    images = images.view(*flipped_images.shape)\n","    mask = mask[:, None] * torch.ones([1, frames]).type(mask.dtype)\n","    \n","    mask = mask.type(images.dtype).to(device)\n","    mask = mask[:, :, None, None]\n","    \n","    out = mask * flipped_images + (1 - mask) * images\n","\n","    out = out.view([bs, -1, h, w])\n","    return out\n","\n","# random rotation\n","\n","def random_rotation(images,p=.3):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: str, cpu or gpu, \n","        p: float, prob of applying aug,\n","        returns torch.tensor\n","    \"\"\"\n","    device = images.device\n","    # images: [B, C, H, W]\n","    bs, channels, h, w = images.shape\n","    \n","    images = images.to(device)\n","\n","    rot90_images = images.rot90(1,[2,3])\n","    rot180_images = images.rot90(2,[2,3])\n","    rot270_images = images.rot90(3,[2,3])    \n","    \n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    rnd_rot = np.random.randint(1, 4, size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = rnd_rot * mask\n","    mask = torch.from_numpy(mask).to(device)\n","    \n","    frames = images.shape[1]\n","    masks = [torch.zeros_like(mask) for _ in range(4)]\n","    for i,m in enumerate(masks):\n","        m[torch.where(mask==i)] = 1\n","        m = m[:, None] * torch.ones([1, frames]).type(mask.dtype).type(images.dtype).to(device)\n","        m = m[:,:,None,None]\n","        masks[i] = m\n","    \n","    \n","    out = masks[0] * images + masks[1] * rot90_images + masks[2] * rot180_images + masks[3] * rot270_images\n","\n","    out = out.view([bs, -1, h, w])\n","    return out\n","\n","\n","# random color\n","\n","    \n","\n","def random_convolution(imgs):\n","    '''\n","    random covolution in \"network randomization\"\n","    \n","    (imbs): B x (C x stack) x H x W, note: imgs should be normalized and torch tensor\n","    '''\n","    _device = imgs.device\n","    \n","    img_h, img_w = imgs.shape[2], imgs.shape[3]\n","    num_stack_channel = imgs.shape[1]\n","    num_batch = imgs.shape[0]\n","    num_trans = num_batch\n","    batch_size = int(num_batch / num_trans)\n","    \n","    # initialize random covolution\n","    rand_conv = nn.Conv2d(3, 3, kernel_size=3, bias=False, padding=1).to(_device)\n","    \n","    for trans_index in range(num_trans):\n","        torch.nn.init.xavier_normal_(rand_conv.weight.data)\n","        temp_imgs = imgs[trans_index*batch_size:(trans_index+1)*batch_size]\n","        temp_imgs = temp_imgs.reshape(-1, 3, img_h, img_w) # (batch x stack, channel, h, w)\n","        rand_out = rand_conv(temp_imgs)\n","        if trans_index == 0:\n","            total_out = rand_out\n","        else:\n","            total_out = torch.cat((total_out, rand_out), 0)\n","    total_out = total_out.reshape(-1, num_stack_channel, img_h, img_w)\n","    return total_out\n","\n","\n","def random_color_jitter(imgs):\n","    \"\"\"\n","        inputs np array outputs tensor\n","    \"\"\"\n","    b,c,h,w = imgs.shape\n","    imgs = imgs.view(-1,3,h,w)\n","    transform_module = nn.Sequential(ColorJitterLayer(brightness=0.4, \n","                                                contrast=0.4,\n","                                                saturation=0.4, \n","                                                hue=0.5, \n","                                                p=1.0, \n","                                                batch_size=b,\n","                                                stack_size=1))\n","\n","    imgs = transform_module(imgs).view(b,c,h,w)\n","    return imgs\n","\n","\n","def random_translate(imgs, size, return_random_idxs=False, h1s=None, w1s=None):\n","    n, c, h, w = imgs.shape\n","    assert size >= h and size >= w\n","    outs = np.zeros((n, c, size, size), dtype=imgs.dtype)\n","    h1s = np.random.randint(0, size - h + 1, n) if h1s is None else h1s\n","    w1s = np.random.randint(0, size - w + 1, n) if w1s is None else w1s\n","    for out, img, h1, w1 in zip(outs, imgs, h1s, w1s):\n","        out[:, h1:h1 + h, w1:w1 + w] = img\n","    if return_random_idxs:  # So can do the same to another set of imgs.\n","        return outs, dict(h1s=h1s, w1s=w1s)\n","    return outs\n","\n","\n","def no_aug(x):\n","    return x\n","\n","\n","# if __name__ == '__main__':\n","#     import time \n","#     from tabulate import tabulate\n","#     def now():\n","#         return time.time()\n","#     def secs(t):\n","#         s = now() - t\n","#         tot = round((1e5 * s)/60,1)\n","#         return round(s,3),tot\n","\n","#     x = np.load('data_sample.npy',allow_pickle=True)\n","#     x = np.concatenate([x,x,x],1)\n","#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#     x = torch.from_numpy(x).to(device)\n","#     x = x.float() / 255.\n","\n","#     # crop\n","#     t = now()\n","#     random_crop(x.cpu().numpy(),64)\n","#     s1,tot1 = secs(t)\n","#     # grayscale \n","#     t = now()\n","#     random_grayscale(x,p=.5)\n","#     s2,tot2 = secs(t)\n","#     # normal cutout \n","#     t = now()\n","#     random_cutout(x.cpu().numpy(),10,30)\n","#     s3,tot3 = secs(t)\n","#     # color cutout \n","#     t = now()\n","#     random_cutout_color(x.cpu().numpy(),10,30)\n","#     s4,tot4 = secs(t)\n","#     # flip \n","#     t = now()\n","#     random_flip(x,p=.5)\n","#     s5,tot5 = secs(t)\n","#     # rotate \n","#     t = now()\n","#     random_rotation(x,p=.5)\n","#     s6,tot6 = secs(t)\n","#     # rand conv \n","#     t = now()\n","#     random_convolution(x)\n","#     s7,tot7 = secs(t)\n","#     # rand color jitter \n","#     t = now()\n","#     random_color_jitter(x)\n","#     s8,tot8 = secs(t)\n","    \n","#     print(tabulate([['Crop', s1,tot1], \n","#                     ['Grayscale', s2,tot2], \n","#                     ['Normal Cutout', s3,tot3], \n","#                     ['Color Cutout', s4,tot4], \n","#                     ['Flip', s5,tot5], \n","#                     ['Rotate', s6,tot6], \n","#                     ['Rand Conv', s7,tot7], \n","#                     ['Color Jitter', s8,tot8]], \n","#                     headers=['Data Aug', 'Time / batch (secs)', 'Time / 100k steps (mins)']))\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bn2rkllGJPtZ"},"source":["Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."]},{"cell_type":"code","metadata":{"id":"8Z8P1ehENCwc","executionInfo":{"status":"ok","timestamp":1607551347891,"user_tz":-60,"elapsed":18032,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":["# Hyperparameters\n","total_steps = 10e6\n","num_envs = 32\n","num_levels = 100\n","num_steps = 256\n","num_epochs = 3\n","batch_size = 512 #512\n","eps = .2\n","grad_eps = .5\n","value_coef = .5\n","entropy_coef = .01"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"_p33_G2yfjoC","executionInfo":{"status":"ok","timestamp":1607551348285,"user_tz":-60,"elapsed":18419,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from utils import make_env, Storage, orthogonal_init\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self,\n","                 in_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        out = nn.ReLU()(x)\n","        out = self.conv1(out)\n","        out = nn.ReLU()(out)\n","        out = self.conv2(out)\n","        return out + x\n","\n","class ImpalaBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(ImpalaBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n","        self.res1 = ResidualBlock(out_channels)\n","        self.res2 = ResidualBlock(out_channels)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n","        x = self.res1(x)\n","        x = self.res2(x)\n","        return x\n","\n","class ImpalaModel(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 **kwargs):\n","        super(ImpalaModel, self).__init__()\n","        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n","        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n","        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n","        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=256)\n","\n","        self.output_dim = 256\n","        self.apply(xavier_uniform_init)\n","\n","    def forward(self, x):\n","        x = self.block1(x)\n","        x = self.block2(x)\n","        x = self.block3(x)\n","        x = nn.ReLU()(x)\n","        x = Flatten()(x)\n","        x = self.fc(x)\n","        x = nn.ReLU()(x)\n","        return x\n","\n","\n","def xavier_uniform_init(module, gain=1.0):\n","    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n","        nn.init.xavier_uniform_(module.weight.data, gain)\n","        nn.init.constant_(module.bias.data, 0)\n","    return module\n","\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","\n","class Encoder(nn.Module):\n","  def __init__(self, in_channels, feature_dim):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n","        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n","        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n","        Flatten(),\n","        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n","    )\n","    self.apply(orthogonal_init)\n","\n","  def forward(self, x):\n","    return self.layers(x)\n","\n","\n","class Policy(nn.Module):\n","  def __init__(self, encoder, feature_dim, num_actions):\n","    super().__init__()\n","    self.encoder = encoder\n","    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n","    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n","\n","  def act(self, x):\n","    with torch.no_grad():\n","      x = x.cuda().contiguous()\n","      dist, value = self.forward(x)\n","      action = dist.sample()\n","      log_prob = dist.log_prob(action)\n","    \n","    return action.cpu(), log_prob.cpu(), value.cpu()\n","\n","  def forward(self, x):\n","    x = self.encoder(x)\n","    logits = self.policy(x)\n","    value = self.value(x).squeeze(1)\n","    dist = torch.distributions.Categorical(logits=logits)\n","\n","    return dist, value\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JxRWy_T9JY4M"},"source":["Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."]},{"cell_type":"code","metadata":{"id":"yTBV9xpKpEFa","colab":{"base_uri":"https://localhost:8080/","height":476},"executionInfo":{"status":"error","timestamp":1607551445433,"user_tz":-60,"elapsed":115557,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}},"outputId":"e0c97bd3-619c-4ec1-ee54-a7b87a825738"},"source":["\n","\n","# Define environmentbossfight\n","# check the utils.py file for info on arguments\n","env = make_env(num_envs, num_levels=num_levels, env_name='bigfish', use_backgrounds=True)\n","print('Observation space:', env.observation_space)\n","print('Action space:', env.action_space.n)\n","\n","# Define network\n","in_channels=env.observation_space.shape[0]\n","encoder =  ImpalaModel(in_channels=in_channels)\n","policy = Policy(encoder, 256, 15)\n","policy.cuda()\n","\n","# Define optimizer\n","# these are reasonable values but probably not optimal\n","optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n","\n","# Define temporary storage\n","# we use this to collect transitions during each iteration\n","storage = Storage(\n","    env.observation_space.shape,\n","    num_steps,\n","    num_envs\n",")\n","\n","# ''' make separate environment for evaluation '''\n","# eval_env = make_env(num_envs, env_name = 'starpilot',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\n","# eval_obs = eval_env.reset()\n","\n","\n","from collections import deque\n","eval_info_queue=deque(maxlen=num_steps)\n","eval_reward_queue=torch.zeros(num_steps, num_envs)\n","\n","# Run training\n","obs = env.reset()\n","step = 0\n","i=0\n","data=[];\n","print(\"NN setup, Training Starts\")\n","while step < total_steps:\n","\n","  policy.eval()\n","  \n","  '''list for storing eval rewards'''\n","  total_reward = []\n","  # Use policy to collect data for num_steps steps\n","  \n","  for _ in range(num_steps):\n","    # Use policy\n","    action, log_prob, value = policy.act(obs)\n","    \n","    # Take step in environment\n","    next_obs, reward, done, info = env.step(action)\n","\n","    # Store data\n","    storage.store(obs, action, reward, done, info, log_prob, value)\n","    \n","    # Update current observation\n","    obs = next_obs\n","\n","\n","  # Add the last observation to collected data\n","  _, _, value = policy.act(obs)\n","  storage.store_last(obs, value)\n","\n","  # Compute return and advantage\n","  storage.compute_return_advantage()\n","\n","  # Optimize policy\n","  policy.train()\n","  for epoch in range(num_epochs):\n","\n","    # Iterate over batches of transitions\n","    generator = storage.get_generator(batch_size)\n","    for batch in generator:\n","      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n","\n","      # apply color jitter\n","      # b_obs = b_obs.to('cpu')\n","      'not sure why doing this but resulting image has 9 channels'\n","      # b_obs=np.concatenate([b_obs,b_obs,b_obs], 1)\n","      # b_obs = torch.from_numpy(b_obs).to('cuda')\n","      \n","      # import torchvision\n","      # ss=torch.squeeze(b_obs)\n","      # from google.colab.patches import cv2_imshow\n","      # cv2_imshow(ss.to('cpu').permute(1, 2, 0).numpy())\n","      \n","      b_obs = random_crop(b_obs.to('cpu').numpy(), out=64)\n","      b_obs=torch.from_numpy(b_obs).to('cuda')\n","      \n","      # Get current policy outputs\n","      new_dist, new_value = policy(b_obs)\n","      new_log_prob = new_dist.log_prob(b_action)\n","      # log_prob\n","      # Clipped policy objective\n","      #print(str(log_prob.shape) + \" \" + str(b_log_prob.shape) + \" \" + str(new_log_prob.shape))\n","      ratio = torch.exp(new_log_prob - b_log_prob)\n","      \n","      clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps) \n","      policy_reward = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)\n","      #clip_fraction = (abs((ratio - 1.0)) > clip).to(torch.float).mean()\n","      pi_loss = -policy_reward.mean()\n","\n","      # Clipped value function objective\n","      # clipped_value = new_value + (b_value - new_value).clamp(min=-eps,max=eps)\n","      # vf_loss=torch.max((b_value-b_returns)**2, (clipped_value-b_returns)**2)\n","      # value_loss = 0.5 * vf_loss.mean()\n","\n","      # clipped_value = b_value + (new_value - b_value).clamp(min=-eps,max=eps) #\n","      # vf_loss=torch.max((new_value-b_returns)**2, (clipped_value-b_returns)**2) #\n","      # value_loss = 0.5 * vf_loss.mean() #\n","      clipped_value = (new_value - b_value).clamp(min=-eps,max=eps)\n","      value_loss = 0.5 * torch.max(torch.pow(new_value - b_returns,2), torch.pow(b_value - b_returns, 2)).mean()\n","\n","      # Entropy loss\n","      entropy_loss = new_dist.entropy().mean()\n","\n","      # Backpropagate losses\n","      # loss = torch.mean(pi_loss+value_coef*value_loss+entropy_coef*entropy_loss) #\n","      loss = pi_loss + value_coef * value_loss - entropy_coef * entropy_loss\n","      loss.backward()\n","\n","      # Clip gradients\n","      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n","\n","      # Update policy\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","  step += num_envs * num_steps\n","  print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n","  data.append(storage.get_reward())\n","  if step%1007616==0:\n","    torch.save(policy.state_dict(), 'checkpointbigaug'+str(i)+'.pt')\n","    i=i+1\n","  if step==8192:\n","    torch.save(policy.state_dict(), 'checkpointbigaug'+str(i)+'.pt')\n","    \n","\n","\n","print('Completed training!')\n","torch.save(policy.state_dict(), 'checkpointFinalbigaug.pt')\n","np.save(\"data.npy\",data)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n","Action space: 15\n","NN setup, Training Starts\n","Step: 8192\tMean reward: 1.40625\n","Step: 16384\tMean reward: 2.1875\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-b4c1952a33bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Take step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Store data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    255\u001b[0m \t\t\"\"\"\n\u001b[1;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                         \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym3/interop.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym3/libenv.py\u001b[0m in \u001b[0;36mget_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibenv_observe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_copy_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"RAZrWuVGLTu-"},"source":["Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."]},{"cell_type":"code","metadata":{"id":"2zecOCkd7Jzt","executionInfo":{"status":"ok","timestamp":1607552433663,"user_tz":-60,"elapsed":258741,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":["import imageio\n","\n","# Make evaluation environment\n","eval_env = make_env(num_envs, env_name = 'bigfish',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\n","obs = eval_env.reset()\n","\n","frames = []\n","eval_reward_lst=[]\n","reward_info_lst=[]\n","reward_lst=[]\n","for i in range(11):\n","  # total_reward = []\n","  info_lst=[]\n","  reward_lst=[]\n","\n","  in_channels=eval_env.observation_space.shape[0]\n","  encoder =  ImpalaModel(in_channels=in_channels)\n","  policy = Policy(encoder, 256, 15)\n","  policy.eval()\n","\n","  if i==10:\n","    policy.load_state_dict(torch.load(\"checkpointFinalbigaug.pt\"))\n","  else:\n","    policy.load_state_dict(torch.load(f\"checkpointbigaug{i}.pt\"))\n","  \n","  # policy.load_state_dict(torch.load(f'checkpointbigaug{i}.pt'))\n","\n","  policy.cuda()\n","  reward_info_lst=[]\n","  total_reward=[]\n","  for _ in range(256):\n","    \n","    # Use policy\n","    action, log_prob, value = policy.act(obs)\n","\n","    # Take step in environment\n","    obs, reward, done, inf = eval_env.step(action)\n","    \n","    # reward_info_lst.append(info['reward'])\n","    # info_lst.append(info)\n","    total_reward.append(torch.Tensor(reward))\n","  \n","  # Calculate average return\n","  total_reward = torch.stack(total_reward).sum(0).mean(0)\n","  eval_reward_lst.append(total_reward)\n","  # info_lst=sum(info_lst)/len(info_lst)\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"zaoopvKZkQg5","colab":{"base_uri":"https://localhost:8080/","height":303},"executionInfo":{"status":"ok","timestamp":1607552434509,"user_tz":-60,"elapsed":256859,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}},"outputId":"279018d3-1b81-4429-86c6-968b0e03aa6c"},"source":["lstlst = [element.cpu().detach().item() for element in eval_reward_lst]\n","# lstlst = \n","print((lstlst))\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","sns.lineplot(data=lstlst)\n","plt.show()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[4.337402820587158, 3.710062026977539, 3.3512067794799805, 3.5229101181030273, 3.8527519702911377, 3.117227077484131, 2.9726645946502686, 4.074331760406494, 2.4618875980377197, 3.2478559017181396, 3.5605227947235107]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"IZdiekuikRBt","executionInfo":{"status":"aborted","timestamp":1607551445432,"user_tz":-60,"elapsed":115493,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":[""],"execution_count":null,"outputs":[]}]}