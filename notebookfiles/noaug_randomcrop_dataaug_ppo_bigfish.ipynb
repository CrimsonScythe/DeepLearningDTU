{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"noaug_randomcrop_dataaug_ppo_bigfish.ipynb","provenance":[{"file_id":"1DI3U0AbU6hYo7SRCy9VP0zvzot-t3Zwo","timestamp":1606939953339},{"file_id":"1kxmUeqO9ZlMnQ-A1fVRu6UflK2CYHWOq","timestamp":1606736133951},{"file_id":"1FPtv9ViOvIp7S5qyDXw1ilRYTgwPwmnY","timestamp":1606396800688},{"file_id":"1Oxz-yvTTmZHiDoPZNpxPOGBlHglllzvs","timestamp":1606295085832},{"file_id":"1hAOVglijseKkHru85r9-RoLl_fZo1sIJ","timestamp":1605725392941},{"file_id":"1LNknRcS0vP5ocaH9fRfV4uvpFgsq6CBo","timestamp":1605540061171},{"file_id":"19eg-qXLO6ywbDl66FJ9AQl46Wjvs1qja","timestamp":1604661124100},{"file_id":"1d4iEmvk2z6v0-M4LtnjIeuXSJPadZ7M8","timestamp":1604306820961},{"file_id":"11nnS8AAIxo5HYcXaSWLo8eGnrePeYOpO","timestamp":1604049801984}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"K1_WKdcrI6w3"},"source":["# Getting started with PPO and ProcGen"]},{"cell_type":"markdown","metadata":{"id":"z7LP1JU3I-d4"},"source":["Here's a bit of code that should help you get started on your projects.\n","\n","The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdpZ4lmFHtD8","executionInfo":{"status":"ok","timestamp":1607551038369,"user_tz":-60,"elapsed":13170,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}},"outputId":"dad42305-fbbd-4d48-ffa3-b3a6fd0846d4"},"source":["!pip install procgen\n","!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n","!wget https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting procgen\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/34/0ae32b01ec623cd822752e567962cfa16ae9c6d6ba2208f3445c017a121b/procgen-0.10.4-cp36-cp36m-manylinux2010_x86_64.whl (39.9MB)\n","\u001b[K     |████████████████████████████████| 39.9MB 81kB/s \n","\u001b[?25hCollecting gym3<1.0.0,>=0.3.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/8c/83da801207f50acfd262041e7974f3b42a0e5edd410149d8a70fd4ad2e70/gym3-0.3.3-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n","\u001b[?25hRequirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (3.0.12)\n","Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (1.18.5)\n","Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.17.3)\n","Collecting glfw<2.0.0,>=1.8.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/1b/cc758368f1b2466b3701c0f692973aa8a0b51a192a40463c1d02d54d640c/glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203kB)\n","\u001b[K     |████████████████████████████████| 204kB 54.7MB/s \n","\u001b[?25hCollecting moderngl<6.0.0,>=5.5.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/ab/5f72a1b7c5bdbb17160c85e8ba855d48925c74ff93c1e1027d5ad40bf33c/moderngl-5.6.2-cp36-cp36m-manylinux1_x86_64.whl (664kB)\n","\u001b[K     |████████████████████████████████| 665kB 56.4MB/s \n","\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.14.3)\n","Collecting imageio-ffmpeg<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/12/01126a2fb737b23461d7dadad3b8abd51ad6210f979ff05c6fa9812dfbbe/imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl (22.2MB)\n","\u001b[K     |████████████████████████████████| 22.2MB 1.4MB/s \n","\u001b[?25hCollecting imageio<3.0.0,>=2.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 51.4MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n","Collecting glcontext<3,>=2\n","  Downloading https://files.pythonhosted.org/packages/b0/8d/93915df9cd8d31c5f054bbacd1c7a76cd2f776b8212dcc768358bd2d4a37/glcontext-2.2.0-cp36-cp36m-manylinux1_x86_64.whl\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.20)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: glfw, glcontext, moderngl, imageio-ffmpeg, imageio, gym3, procgen\n","  Found existing installation: imageio 2.4.1\n","    Uninstalling imageio-2.4.1:\n","      Successfully uninstalled imageio-2.4.1\n","Successfully installed glcontext-2.2.0 glfw-1.12.0 gym3-0.3.3 imageio-2.9.0 imageio-ffmpeg-0.3.0 moderngl-5.6.2 procgen-0.10.4\n","--2020-12-09 21:57:18--  https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14807 (14K) [text/plain]\n","Saving to: ‘utils.py’\n","\n","utils.py            100%[===================>]  14.46K  --.-KB/s    in 0s      \n","\n","2020-12-09 21:57:18 (94.8 MB/s) - ‘utils.py’ saved [14807/14807]\n","\n","--2020-12-09 21:57:18--  https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7657 (7.5K) [text/plain]\n","Saving to: ‘TransformLayer.py’\n","\n","TransformLayer.py   100%[===================>]   7.48K  --.-KB/s    in 0s      \n","\n","2020-12-09 21:57:18 (106 MB/s) - ‘TransformLayer.py’ saved [7657/7657]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yDfs7DVPvlhp"},"source":["Data aug code from\n","https://github.com/MishaLaskin/rad/blob/1246bfd6e716669126e12c1f02f393801e1692c1/data_augs.py#L296\n"]},{"cell_type":"code","metadata":{"id":"rhVaMUy7ukKz","executionInfo":{"status":"ok","timestamp":1607551042487,"user_tz":-60,"elapsed":17271,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["'''\n","dataaugs:\n","https://github.com/MishaLaskin/rad/blob/1246bfd6e716669126e12c1f02f393801e1692c1/data_augs.py#L296\n","'''\n","'''\n","paper:\n","https://arxiv.org/pdf/2004.14990.pdf\n","'''\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from TransformLayer import ColorJitterLayer\n","\n","\n","def random_crop(imgs, out=84):\n","    \"\"\"\n","        args:\n","        imgs: np.array shape (B,C,H,W)\n","        out: output size (e.g. 84)\n","        returns np.array\n","    \"\"\"\n","    n, c, h, w = imgs.shape\n","    \n","    crop_max = h - out + 1\n","    w1 = np.random.randint(0, crop_max, n)\n","    h1 = np.random.randint(0, crop_max, n)\n","    cropped = np.empty((c, out, out), dtype=imgs.dtype)\n","    cropped = imgs[:, h1+out, w1+out]\n","    cropped = np.empty((n, c, out, out), dtype=imgs.dtype)\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        \n","        cropped[i] = img[:, h11:h11 + out, w11:w11 + out]\n","    return cropped\n","\n","\n","def grayscale(imgs):\n","    # imgs: b x c x h x w\n","    device = imgs.device\n","    b, c, h, w = imgs.shape\n","    frames = c // 3\n","    \n","    imgs = imgs.view([b,frames,3,h,w])\n","    imgs = imgs[:, :, 0, ...] * 0.2989 + imgs[:, :, 1, ...] * 0.587 + imgs[:, :, 2, ...] * 0.114 \n","    \n","    imgs = imgs.type(torch.uint8).float()\n","    # assert len(imgs.shape) == 3, imgs.shape\n","    imgs = imgs[:, :, None, :, :]\n","    imgs = imgs * torch.ones([1, 1, 3, 1, 1], dtype=imgs.dtype).float().to(device) # broadcast tiling\n","    return imgs\n","\n","def random_grayscale(images,p=.3):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: cpu or cuda\n","        returns torch.tensor\n","    \"\"\"\n","    device = images.device\n","    in_type = images.type()\n","    images = images * 255.\n","    images = images.type(torch.uint8)\n","    # images: [B, C, H, W]\n","    bs, channels, h, w = images.shape\n","    images = images.to(device)\n","    gray_images = grayscale(images)\n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = torch.from_numpy(mask)\n","    frames = images.shape[1] // 3\n","    images = images.view(*gray_images.shape)\n","    mask = mask[:, None] * torch.ones([1, frames]).type(mask.dtype)\n","    mask = mask.type(images.dtype).to(device)\n","    mask = mask[:, :, None, None, None]\n","    out = mask * gray_images + (1 - mask) * images\n","    out = out.view([bs, -1, h, w]).type(in_type) / 255.\n","    return out\n","\n","# random cutout\n","# TODO: should mask this \n","\n","def random_cutout(imgs, min_cut=10,max_cut=30):\n","    \"\"\"\n","        args:\n","        imgs: np.array shape (B,C,H,W)\n","        min / max cut: int, min / max size of cutout \n","        returns np.array\n","    \"\"\"\n","\n","    n, c, h, w = imgs.shape\n","    w1 = np.random.randint(min_cut, max_cut, n)\n","    h1 = np.random.randint(min_cut, max_cut, n)\n","    \n","    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        cut_img = img.copy()\n","        cut_img[:, h11:h11 + h11, w11:w11 + w11] = 0\n","        #print(img[:, h11:h11 + h11, w11:w11 + w11].shape)\n","        cutouts[i] = cut_img\n","    return cutouts\n","\n","def random_cutout_color(imgs, min_cut=100,max_cut=250):\n","    \"\"\"\n","        args:\n","        imgs: shape (B,C,H,W)\n","        out: output size (e.g. 84)\n","    \"\"\"\n","    n, c, h, w = imgs.shape\n","    w1 = np.random.randint(min_cut, max_cut, n)\n","    h1 = np.random.randint(min_cut, max_cut, n)\n","    \n","    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n","    rand_box = np.random.randint(0, 255, size=(n, c)) / 255.\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        \n","        cut_img = img.copy()\n","        \n","        # add random box\n","        cut_img[:, h11:h11 + h11, w11:w11 + w11] = np.tile(\n","            rand_box[i].reshape(-1,1,1),                                                \n","            (1,) + cut_img[:, h11:h11 + h11, w11:w11 + w11].shape[1:])\n","        \n","        cutouts[i] = cut_img\n","    return cutouts\n","    \n","    # n, c, h, w = imgs.shape\n","    # w1 = np.random.randint(min_cut, max_cut, n)\n","    # h1 = np.random.randint(min_cut, max_cut, n)\n","    \n","    # cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n","    # rand_box = np.random.randint(0, 255, size=(n, c)) / 255.\n","    # for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","    #     cut_img = img.copy()\n","        \n","    #     # add random box\n","    #     cut_img[:, h11:h11 + h11, w11:w11 + w11] = np.tile(\n","    #         rand_box[i].reshape(-1,1,1),                                                \n","    #         (1,) + cut_img[:, h11:h11 + h11, w11:w11 + w11].shape[1:])\n","        \n","    #     cutouts[i] = cut_img\n","    # return cutouts\n","\n","# random flip\n","\n","def random_flip(images,p=.2):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: cpu or gpu, \n","        p: prob of applying aug,\n","        returns torch.tensor\n","    \"\"\"\n","    # images: [B, C, H, W]\n","    device = images.device\n","    bs, channels, h, w = images.shape\n","    \n","    images = images.to(device)\n","\n","    flipped_images = images.flip([3])\n","    \n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = torch.from_numpy(mask)\n","    frames = images.shape[1] #// 3\n","    images = images.view(*flipped_images.shape)\n","    mask = mask[:, None] * torch.ones([1, frames]).type(mask.dtype)\n","    \n","    mask = mask.type(images.dtype).to(device)\n","    mask = mask[:, :, None, None]\n","    \n","    out = mask * flipped_images + (1 - mask) * images\n","\n","    out = out.view([bs, -1, h, w])\n","    return out\n","\n","# random rotation\n","\n","def random_rotation(images,p=.3):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: str, cpu or gpu, \n","        p: float, prob of applying aug,\n","        returns torch.tensor\n","    \"\"\"\n","    device = images.device\n","    # images: [B, C, H, W]\n","    bs, channels, h, w = images.shape\n","    \n","    images = images.to(device)\n","\n","    rot90_images = images.rot90(1,[2,3])\n","    rot180_images = images.rot90(2,[2,3])\n","    rot270_images = images.rot90(3,[2,3])    \n","    \n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    rnd_rot = np.random.randint(1, 4, size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = rnd_rot * mask\n","    mask = torch.from_numpy(mask).to(device)\n","    \n","    frames = images.shape[1]\n","    masks = [torch.zeros_like(mask) for _ in range(4)]\n","    for i,m in enumerate(masks):\n","        m[torch.where(mask==i)] = 1\n","        m = m[:, None] * torch.ones([1, frames]).type(mask.dtype).type(images.dtype).to(device)\n","        m = m[:,:,None,None]\n","        masks[i] = m\n","    \n","    \n","    out = masks[0] * images + masks[1] * rot90_images + masks[2] * rot180_images + masks[3] * rot270_images\n","\n","    out = out.view([bs, -1, h, w])\n","    return out\n","\n","\n","# random color\n","\n","    \n","\n","def random_convolution(imgs):\n","    '''\n","    random covolution in \"network randomization\"\n","    \n","    (imbs): B x (C x stack) x H x W, note: imgs should be normalized and torch tensor\n","    '''\n","    _device = imgs.device\n","    \n","    img_h, img_w = imgs.shape[2], imgs.shape[3]\n","    num_stack_channel = imgs.shape[1]\n","    num_batch = imgs.shape[0]\n","    num_trans = num_batch\n","    batch_size = int(num_batch / num_trans)\n","    \n","    # initialize random covolution\n","    rand_conv = nn.Conv2d(3, 3, kernel_size=3, bias=False, padding=1).to(_device)\n","    \n","    for trans_index in range(num_trans):\n","        torch.nn.init.xavier_normal_(rand_conv.weight.data)\n","        temp_imgs = imgs[trans_index*batch_size:(trans_index+1)*batch_size]\n","        temp_imgs = temp_imgs.reshape(-1, 3, img_h, img_w) # (batch x stack, channel, h, w)\n","        rand_out = rand_conv(temp_imgs)\n","        if trans_index == 0:\n","            total_out = rand_out\n","        else:\n","            total_out = torch.cat((total_out, rand_out), 0)\n","    total_out = total_out.reshape(-1, num_stack_channel, img_h, img_w)\n","    return total_out\n","\n","\n","def random_color_jitter(imgs):\n","    \"\"\"\n","        inputs np array outputs tensor\n","    \"\"\"\n","    b,c,h,w = imgs.shape\n","    imgs = imgs.view(-1,3,h,w)\n","    transform_module = nn.Sequential(ColorJitterLayer(brightness=0.4, \n","                                                contrast=0.4,\n","                                                saturation=0.4, \n","                                                hue=0.5, \n","                                                p=1.0, \n","                                                batch_size=b,\n","                                                stack_size=1))\n","\n","    imgs = transform_module(imgs).view(b,c,h,w)\n","    return imgs\n","\n","\n","def random_translate(imgs, size, return_random_idxs=False, h1s=None, w1s=None):\n","    n, c, h, w = imgs.shape\n","    assert size >= h and size >= w\n","    outs = np.zeros((n, c, size, size), dtype=imgs.dtype)\n","    h1s = np.random.randint(0, size - h + 1, n) if h1s is None else h1s\n","    w1s = np.random.randint(0, size - w + 1, n) if w1s is None else w1s\n","    for out, img, h1, w1 in zip(outs, imgs, h1s, w1s):\n","        out[:, h1:h1 + h, w1:w1 + w] = img\n","    if return_random_idxs:  # So can do the same to another set of imgs.\n","        return outs, dict(h1s=h1s, w1s=w1s)\n","    return outs\n","\n","\n","def no_aug(x):\n","    return x\n","\n","\n","# if __name__ == '__main__':\n","#     import time \n","#     from tabulate import tabulate\n","#     def now():\n","#         return time.time()\n","#     def secs(t):\n","#         s = now() - t\n","#         tot = round((1e5 * s)/60,1)\n","#         return round(s,3),tot\n","\n","#     x = np.load('data_sample.npy',allow_pickle=True)\n","#     x = np.concatenate([x,x,x],1)\n","#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#     x = torch.from_numpy(x).to(device)\n","#     x = x.float() / 255.\n","\n","#     # crop\n","#     t = now()\n","#     random_crop(x.cpu().numpy(),64)\n","#     s1,tot1 = secs(t)\n","#     # grayscale \n","#     t = now()\n","#     random_grayscale(x,p=.5)\n","#     s2,tot2 = secs(t)\n","#     # normal cutout \n","#     t = now()\n","#     random_cutout(x.cpu().numpy(),10,30)\n","#     s3,tot3 = secs(t)\n","#     # color cutout \n","#     t = now()\n","#     random_cutout_color(x.cpu().numpy(),10,30)\n","#     s4,tot4 = secs(t)\n","#     # flip \n","#     t = now()\n","#     random_flip(x,p=.5)\n","#     s5,tot5 = secs(t)\n","#     # rotate \n","#     t = now()\n","#     random_rotation(x,p=.5)\n","#     s6,tot6 = secs(t)\n","#     # rand conv \n","#     t = now()\n","#     random_convolution(x)\n","#     s7,tot7 = secs(t)\n","#     # rand color jitter \n","#     t = now()\n","#     random_color_jitter(x)\n","#     s8,tot8 = secs(t)\n","    \n","#     print(tabulate([['Crop', s1,tot1], \n","#                     ['Grayscale', s2,tot2], \n","#                     ['Normal Cutout', s3,tot3], \n","#                     ['Color Cutout', s4,tot4], \n","#                     ['Flip', s5,tot5], \n","#                     ['Rotate', s6,tot6], \n","#                     ['Rand Conv', s7,tot7], \n","#                     ['Color Jitter', s8,tot8]], \n","#                     headers=['Data Aug', 'Time / batch (secs)', 'Time / 100k steps (mins)']))\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bn2rkllGJPtZ"},"source":["Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."]},{"cell_type":"code","metadata":{"id":"8Z8P1ehENCwc","executionInfo":{"status":"ok","timestamp":1607551042490,"user_tz":-60,"elapsed":17264,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["# Hyperparameters\n","total_steps = 10e6\n","num_envs = 32\n","num_levels = 100\n","num_steps = 256\n","num_epochs = 3\n","batch_size = 1 #512\n","eps = .2\n","grad_eps = .5\n","value_coef = .5\n","entropy_coef = .01"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"_p33_G2yfjoC","executionInfo":{"status":"ok","timestamp":1607551043262,"user_tz":-60,"elapsed":18025,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from utils import make_env, Storage, orthogonal_init\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self,\n","                 in_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        out = nn.ReLU()(x)\n","        out = self.conv1(out)\n","        out = nn.ReLU()(out)\n","        out = self.conv2(out)\n","        return out + x\n","\n","class ImpalaBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(ImpalaBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n","        self.res1 = ResidualBlock(out_channels)\n","        self.res2 = ResidualBlock(out_channels)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n","        x = self.res1(x)\n","        x = self.res2(x)\n","        return x\n","\n","class ImpalaModel(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 **kwargs):\n","        super(ImpalaModel, self).__init__()\n","        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n","        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n","        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n","        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=256)\n","\n","        self.output_dim = 256\n","        self.apply(xavier_uniform_init)\n","\n","    def forward(self, x):\n","        x = self.block1(x)\n","        x = self.block2(x)\n","        x = self.block3(x)\n","        x = nn.ReLU()(x)\n","        x = Flatten()(x)\n","        x = self.fc(x)\n","        x = nn.ReLU()(x)\n","        return x\n","\n","\n","def xavier_uniform_init(module, gain=1.0):\n","    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n","        nn.init.xavier_uniform_(module.weight.data, gain)\n","        nn.init.constant_(module.bias.data, 0)\n","    return module\n","\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","\n","class Encoder(nn.Module):\n","  def __init__(self, in_channels, feature_dim):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n","        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n","        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n","        Flatten(),\n","        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n","    )\n","    self.apply(orthogonal_init)\n","\n","  def forward(self, x):\n","    return self.layers(x)\n","\n","\n","class Policy(nn.Module):\n","  def __init__(self, encoder, feature_dim, num_actions):\n","    super().__init__()\n","    self.encoder = encoder\n","    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n","    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n","\n","  def act(self, x):\n","    with torch.no_grad():\n","      x = x.cuda().contiguous()\n","      dist, value = self.forward(x)\n","      action = dist.sample()\n","      log_prob = dist.log_prob(action)\n","    \n","    return action.cpu(), log_prob.cpu(), value.cpu()\n","\n","  def forward(self, x):\n","    x = self.encoder(x)\n","    logits = self.policy(x)\n","    value = self.value(x).squeeze(1)\n","    dist = torch.distributions.Categorical(logits=logits)\n","\n","    return dist, value\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"aejMK-62h7iH","executionInfo":{"status":"ok","timestamp":1607551043265,"user_tz":-60,"elapsed":18015,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["# from PIL import Image\r\n","\r\n","# import cv2\r\n","#       # im = Image.open('big.png')\r\n","#       # im.load()\r\n","#       # im = np.asarray( im, dtype=\"int32\" )\r\n","#       # print(im.shape)\r\n","    \r\n","#       # im = random_crop(im, out=306)\r\n","     \r\n","#       # from PIL import Image\r\n","\r\n","#       # img = Image.fromarray(im, 'RGB')\r\n","#       # img.save('my.png')\r\n","#       # display(img)\r\n","\r\n","\r\n","# from matplotlib.image import imread\r\n","\r\n","# # im = imread('big.png')\r\n","# # print(im.shape)\r\n","# # im = np.transpose(im, (2, 0, 1))\r\n","# # print(im.shape)\r\n","\r\n","# from PIL import Image\r\n","# from numpy import asarray\r\n","# # load the image\r\n","# image = Image.open('big2.png')\r\n","# # convert image to numpy array\r\n","# data = asarray(image)\r\n","# print(data.shape)\r\n","# # data = np.transpose(data, (2, 0, 1))\r\n","\r\n","\r\n","# data = data[np.newaxis, :, :, :]\r\n","# # print(im.shape)\r\n","# data = random_crop(data, out=20)\r\n","# print('ss', data.shape)\r\n","\r\n","# from PIL import Image\r\n","\r\n","# # data = data[0, :, :, :]\r\n","# data=data[0]\r\n","# print('af', data.shape)\r\n","# # im=im[0]\r\n","\r\n","# image2 = Image.fromarray(data)\r\n","\r\n","# # im = im[0, :, :, :]\r\n","# # im=im[0]\r\n","\r\n","\r\n","# # img.save('my2.png')\r\n","# display(image2)\r\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JxRWy_T9JY4M"},"source":["Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":459},"id":"yTBV9xpKpEFa","executionInfo":{"status":"error","timestamp":1607551448455,"user_tz":-60,"elapsed":423194,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}},"outputId":"0b0cdf54-2e65-4813-e3ce-d716d6713968"},"source":["\n","\n","# Define environmentbossfight\n","# check the utils.py file for info on arguments\n","env = make_env(num_envs, num_levels=num_levels, env_name='bigfish', use_backgrounds=True)\n","print('Observation space:', env.observation_space)\n","print('Action space:', env.action_space.n)\n","\n","# Define network\n","in_channels=env.observation_space.shape[0]\n","encoder =  ImpalaModel(in_channels=in_channels)\n","policy = Policy(encoder, 256, 15)\n","policy.cuda()\n","\n","# Define optimizer\n","# these are reasonable values but probably not optimal\n","optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n","\n","# Define temporary storage\n","# we use this to collect transitions during each iteration\n","storage = Storage(\n","    env.observation_space.shape,\n","    num_steps,\n","    num_envs\n",")\n","\n","# ''' make separate environment for evaluation '''\n","# eval_env = make_env(num_envs, env_name = 'starpilot',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\n","# eval_obs = eval_env.reset()\n","\n","\n","from collections import deque\n","eval_info_queue=deque(maxlen=num_steps)\n","eval_reward_queue=torch.zeros(num_steps, num_envs)\n","\n","# Run training\n","obs = env.reset()\n","step = 0\n","i=0\n","data=[];\n","print(\"NN setup, Training Starts\")\n","while step < total_steps:\n","\n","  policy.eval()\n","  \n","  '''list for storing eval rewards'''\n","  total_reward = []\n","  # Use policy to collect data for num_steps steps\n","  \n","  for _ in range(num_steps):\n","    # Use policy\n","    action, log_prob, value = policy.act(obs)\n","    \n","    # Take step in environment\n","    next_obs, reward, done, info = env.step(action)\n","\n","    # Store data\n","    storage.store(obs, action, reward, done, info, log_prob, value)\n","    \n","    # Update current observation\n","    obs = next_obs\n","\n","    \n","  # Add the last observation to collected data\n","  _, _, value = policy.act(obs)\n","  storage.store_last(obs, value)\n","\n","  # Compute return and advantage\n","  storage.compute_return_advantage()\n","\n","  # Optimize policy\n","  policy.train()\n","  for epoch in range(num_epochs):\n","\n","    # Iterate over batches of transitions\n","    generator = storage.get_generator(batch_size)\n","    for batch in generator:\n","      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n","\n","      # apply color jitter\n","      # b_obs = b_obs.to('cpu')\n","      'not sure why doing this but resulting image has 9 channels'\n","      # b_obs=np.concatenate([b_obs,b_obs,b_obs], 1)\n","      # b_obs = torch.from_numpy(b_obs).to('cuda')\n","      \n","      # import torchvision\n","      # ss=torch.squeeze(b_obs)\n","      # from google.colab.patches import cv2_imshow\n","      # cv2_imshow(ss.to('cpu').permute(1, 2, 0).numpy())\n","      # from PIL import Image\n","\n","      # import cv2\n","      # im = Image.open('big.png')\n","      # im.load()\n","      # im = np.asarray( im, dtype=\"int32\" )\n","      # print(im.shape)\n","    \n","      # im = random_crop(im, out=306)\n","     \n","      # from PIL import Image\n","\n","      # img = Image.fromarray(im, 'RGB')\n","      # img.save('my.png')\n","      # display(img)\n","\n","\n","      # from matplotlib.image import imread\n","\n","      # im = imread('big.png')\n","      # im = np.transpose(im, (2, 0, 1))\n","      # im = im[np.newaxis, :, :, :]\n","      # print(im.shape)\n","      # im = random_cutout_color(im)\n","      # im = random_crop(im, out=306)\n","\n","      # from PIL import Image\n","\n","      # im = im[0, :, :, :]\n","      # im=im[0]\n","\n","      # img = Image.fromarray(im, 'RGB')\n","      # img.save('my2.png')\n","      # display(img)\n","\n","      # b_obs=torch.from_numpy(b_obs).to('cuda')\n","      # Get current policy outputs\n","\n","      new_dist, new_value = policy(b_obs)\n","      new_log_prob = new_dist.log_prob(b_action)\n","      # log_prob\n","      # Clipped policy objective\n","      #print(str(log_prob.shape) + \" \" + str(b_log_prob.shape) + \" \" + str(new_log_prob.shape))\n","      ratio = torch.exp(new_log_prob - b_log_prob)\n","      \n","      clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps) \n","      policy_reward = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)\n","      #clip_fraction = (abs((ratio - 1.0)) > clip).to(torch.float).mean()\n","      pi_loss = -policy_reward.mean()\n","\n","      # Clipped value function objective\n","      # clipped_value = new_value + (b_value - new_value).clamp(min=-eps,max=eps)\n","      # vf_loss=torch.max((b_value-b_returns)**2, (clipped_value-b_returns)**2)\n","      # value_loss = 0.5 * vf_loss.mean()\n","\n","      # clipped_value = b_value + (new_value - b_value).clamp(min=-eps,max=eps) #\n","      # vf_loss=torch.max((new_value-b_returns)**2, (clipped_value-b_returns)**2) #\n","      # value_loss = 0.5 * vf_loss.mean() #\n","      clipped_value = (new_value - b_value).clamp(min=-eps,max=eps)\n","      value_loss = 0.5 * torch.max(torch.pow(new_value - b_returns,2), torch.pow(b_value - b_returns, 2)).mean()\n","\n","      # Entropy loss\n","      entropy_loss = new_dist.entropy().mean()\n","\n","      # Backpropagate losses\n","      # loss = torch.mean(pi_loss+value_coef*value_loss+entropy_coef*entropy_loss) #\n","      loss = pi_loss + value_coef * value_loss - entropy_coef * entropy_loss\n","      loss.backward()\n","\n","      # Clip gradients\n","      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n","\n","      # Update policy\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","  # Update stats\n","  step += num_envs * num_steps\n","  print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n","  data.append(storage.get_reward())\n","\n","  if step%1007616==0:\n","    torch.save(policy.state_dict(), 'checknoaugbig'+str(i)+'.pt')\n","    i=i+1\n","  if step==8192:\n","    torch.save(policy.state_dict(), 'checknoaugbig11'+str(i)+'.pt')\n","\n","\n","print('Completed training!')\n","torch.save(policy.state_dict(), 'checknoaugFinalbig.pt')\n","np.save(\"databig.npy\",data)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n","Action space: 15\n","NN setup, Training Starts\n","Step: 8192\tMean reward: 1.40625\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-bdfdfb578c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0;31m# Get current policy outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0mnew_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0mnew_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0;31m# log_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-654e422ea63e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-654e422ea63e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    717\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 self._forward_pre_hooks.values()):\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"RAZrWuVGLTu-"},"source":["Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."]},{"cell_type":"code","metadata":{"id":"2zecOCkd7Jzt","executionInfo":{"status":"ok","timestamp":1607552173041,"user_tz":-60,"elapsed":257189,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["import imageio\n","\n","# Make evaluation environment\n","eval_env = make_env(num_envs, env_name = 'bigfish',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\n","obs = eval_env.reset()\n","\n","frames = []\n","eval_reward_lst=[]\n","reward_info_lst=[]\n","reward_lst=[]\n","for i in range(11):\n","  # total_reward = []\n","  info_lst=[]\n","  reward_lst=[]\n","  \n","  in_channels=eval_env.observation_space.shape[0]\n","  encoder =  ImpalaModel(in_channels=in_channels)\n","  policy = Policy(encoder, 256, 15)\n","  policy.eval()\n","  \n","  # encoder = Encoder(3,512)\n","  # policy = Policy(encoder, 512, 15)\n","  # policy.eval()\n","  if i==10:\n","    policy.load_state_dict(torch.load(\"checknoaugFinalbig.pt\"))\n","  else:\n","    policy.load_state_dict(torch.load(f\"checknoaugbig{i}.pt\"))\n","  # policy.load_state_dict(torch.load(f\"checknoaugbig{i}.pt\"))\n","\n","  policy.cuda()\n","  reward_info_lst=[]\n","  total_reward=[]\n","  for _ in range(256):\n","    \n","    # Use policy\n","    action, log_prob, value = policy.act(obs)\n","\n","    # Take step in environment\n","    obs, reward, done, inf = eval_env.step(action)\n","    \n","    # reward_info_lst.append(info['reward'])\n","    # info_lst.append(info)\n","    total_reward.append(torch.Tensor(reward))\n","  \n","  # Calculate average return\n","  total_reward = torch.stack(total_reward).sum(0).mean(0)\n","  eval_reward_lst.append(total_reward)\n","  # info_lst=sum(info_lst)/len(info_lst)\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"37qB1OWewf4R","colab":{"base_uri":"https://localhost:8080/","height":303},"executionInfo":{"status":"ok","timestamp":1607552173473,"user_tz":-60,"elapsed":253587,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}},"outputId":"3ce8ca70-e887-48e8-fca4-b1d200599fd1"},"source":["lstlst = [element.cpu().detach().item() for element in eval_reward_lst]\n","# lstlst = \n","print((lstlst))\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","sns.lineplot(data=lstlst)\n","plt.show()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[4.835547924041748, 4.9629034996032715, 3.923645257949829, 3.957695245742798, 4.616264343261719, 2.9919159412384033, 3.3667919635772705, 3.8513553142547607, 3.144526243209839, 2.72218656539917, 3.7005295753479004]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"yGPMzsZCp5xO","executionInfo":{"status":"aborted","timestamp":1607551448451,"user_tz":-60,"elapsed":423164,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["# # !pip install kora\n","# # from kora import console\n","# # console.start()\n","# !pip install procgen\n","# !wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n","# !wget https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py\n","# # Hyperparameters\n","# total_steps = 8e6\n","# num_envs = 32\n","# num_levels = 100\n","# num_steps = 256\n","# num_epochs = 3\n","# batch_size = 512 #512\n","# eps = .2\n","# grad_eps = .5\n","# value_coef = .5\n","# entropy_coef = .01"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"saDiq53q2TZl","executionInfo":{"status":"aborted","timestamp":1607551448452,"user_tz":-60,"elapsed":423158,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# from utils import make_env, Storage, orthogonal_init\n","# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# from utils import make_env, Storage, orthogonal_init\n","  \n","\n","# class Flatten(nn.Module):\n","#     def forward(self, x):\n","#         return x.view(x.size(0), -1)\n","\n","\n","# class Encoder(nn.Module):\n","#   def __init__(self, in_channels, feature_dim):\n","#     super().__init__()\n","#     self.layers = nn.Sequential(\n","#         nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n","#         nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n","#         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n","#         Flatten(),\n","#         nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n","#     )\n","#     self.apply(orthogonal_init)\n","\n","#   def forward(self, x):\n","#     return self.layers(x)\n","\n","\n","# class Policy(nn.Module):\n","#   def __init__(self, encoder, feature_dim, num_actions):\n","#     super().__init__()\n","#     self.encoder = encoder\n","#     self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n","#     self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n","\n","#   def act(self, x):\n","#     with torch.no_grad():\n","#       x = x.cuda().contiguous()\n","#       dist, value = self.forward(x)\n","#       action = dist.sample()\n","#       log_prob = dist.log_prob(action)\n","    \n","#     return action.cpu(), log_prob.cpu(), value.cpu()\n","\n","#   def forward(self, x):\n","#     x = self.encoder(x)\n","#     logits = self.policy(x)\n","#     value = self.value(x).squeeze(1)\n","#     dist = torch.distributions.Categorical(logits=logits)\n","\n","#     return dist, value\n","\n","\n","# # Define environmentbossfight\n","# # check the utils.py file for info on arguments\n","# eval_env = make_env(num_envs, env_name = 'starpilot',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\n","# obs = eval_env.reset()\n","# total_reward = []\n","\n","# # Define network\n","# encoder = Encoder(3,512)\n","# policy = Policy(encoder, 512, 15)\n","# policy.cuda()\n","# policy.load_state_dict(torch.load('checkpoint.pt'))\n","# policy.eval()\n","\n","\n","\n","# frames = []\n","# total_reward = []\n","\n","# # Evaluate policy\n","# policy.eval()#512\n","# for _ in range(512):\n","\n","#   # Use policy\n","#   action, log_prob, value = policy.act(obs)\n","\n","#   # Take step in environment\n","#   obs, reward, done, info = eval_env.step(action)\n","#   total_reward.append(torch.Tensor(reward))\n","\n","#   # Render environment and store\n","#   frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n","#   frames.append(frame)\n","\n","# # Calculate average return\n","# # total_reward = torch.stack(total_reward).sum(0).mean(0)\n","# # print('Average return:', total_reward)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7kjOfWm48RO","executionInfo":{"status":"aborted","timestamp":1607551448453,"user_tz":-60,"elapsed":423152,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["# print(len(total_reward))\n","# print(len(total_reward[0]))\n","# ree=0.0\n","# reet=[]\n","# for i in range(len(total_reward)):\n","#     ree+=total_reward[i].mean()\n","#     reet.append(total_reward[i].mean())\n","# print(ree)\n","# print(reet)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSobD1J74OWZ","executionInfo":{"status":"aborted","timestamp":1607551448454,"user_tz":-60,"elapsed":423150,"user":{"displayName":"Crimson Scythe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5YhYZZnuEHi2VZkAlStFjE7_YgNaUtzkOGVFQ4w=s64","userId":"06715690238472200405"}}},"source":["# import imageio\r\n","\r\n","# # Make evaluation environment\r\n","# eval_env = make_env(num_envs, env_name = 'bigfish',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\r\n","# obs = eval_env.reset()\r\n","\r\n","# frames = []\r\n","# eval_reward_lst=[]\r\n","# reward_info_lst=[]\r\n","# reward_lst=[]\r\n","# for i in range(10):\r\n","#   # total_reward = []\r\n","#   info_lst=[]\r\n","#   reward_lst=[]\r\n","  \r\n","#   in_channels=eval_env.observation_space.shape[0]\r\n","#   encoder =  ImpalaModel(in_channels=in_channels)\r\n","#   policy = Policy(encoder, 256, 15)\r\n","#   policy.eval()\r\n","  \r\n","#   # encoder = Encoder(3,512)\r\n","#   # policy = Policy(encoder, 512, 15)\r\n","#   # policy.eval()\r\n","#   if i==9:\r\n","#     policy.load_state_dict(torch.load(\"checknoaugFinalbig.pt\"))\r\n","#   else:\r\n","#     policy.load_state_dict(torch.load(f\"checknoaugbig{i}.pt\"))\r\n","\r\n","#   policy.cuda()\r\n","#   reward_info_lst=[]\r\n","#   total_reward=[]\r\n","#   for _ in range(256):\r\n","    \r\n","#     # Use policy\r\n","#     action, log_prob, value = policy.act(obs)\r\n","\r\n","#     # Take step in environment\r\n","#     obs, reward, done, inf = eval_env.step(action)\r\n","    \r\n","#     # reward_info_lst.append(info['reward'])\r\n","#     # info_lst.append(info)\r\n","#     total_reward.append(torch.Tensor(reward))\r\n","  \r\n","#   # Calculate average return\r\n","#   total_reward = torch.stack(total_reward).sum(0).mean(0)\r\n","#   eval_reward_lst.append(total_reward)\r\n","#   # info_lst=sum(info_lst)/len(info_lst)\r\n"],"execution_count":null,"outputs":[]}]}