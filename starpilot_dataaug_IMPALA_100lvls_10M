{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"starpilot_dataaug_IMPALA_100lvls_10M","provenance":[{"file_id":"1_OFUT7JSHPhTaGyzVK3rRKadxf9iai__","timestamp":1607170854642},{"file_id":"1LNknRcS0vP5ocaH9fRfV4uvpFgsq6CBo","timestamp":1605533980879},{"file_id":"19eg-qXLO6ywbDl66FJ9AQl46Wjvs1qja","timestamp":1604661124100},{"file_id":"1d4iEmvk2z6v0-M4LtnjIeuXSJPadZ7M8","timestamp":1604306820961},{"file_id":"11nnS8AAIxo5HYcXaSWLo8eGnrePeYOpO","timestamp":1604049801984}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K1_WKdcrI6w3"},"source":["# Getting started with PPO and ProcGen"]},{"cell_type":"markdown","metadata":{"id":"z7LP1JU3I-d4"},"source":["Here's a bit of code that should help you get started on your projects.\n","\n","The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdpZ4lmFHtD8","executionInfo":{"status":"ok","timestamp":1607591884668,"user_tz":-60,"elapsed":4798,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}},"outputId":"eed6abf6-e610-4048-aee7-87e6c2e39bff"},"source":["!pip install procgen\n","!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n","!wget https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting procgen\n","  Using cached https://files.pythonhosted.org/packages/d6/34/0ae32b01ec623cd822752e567962cfa16ae9c6d6ba2208f3445c017a121b/procgen-0.10.4-cp36-cp36m-manylinux2010_x86_64.whl\n","Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (1.18.5)\n","Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.3.3)\n","Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (3.0.12)\n","Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.17.3)\n","Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n","Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.14.3)\n","Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.9.0)\n","Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.6.2)\n","Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.20)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.0.0)\n","Requirement already satisfied: glcontext<3,>=2 in /usr/local/lib/python3.6/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (2.2.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n","Installing collected packages: procgen\n","Successfully installed procgen-0.10.4\n","--2020-12-10 09:18:05--  https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14807 (14K) [text/plain]\n","Saving to: ‘utils.py.1’\n","\n","utils.py.1          100%[===================>]  14.46K  --.-KB/s    in 0s      \n","\n","2020-12-10 09:18:05 (108 MB/s) - ‘utils.py.1’ saved [14807/14807]\n","\n","--2020-12-10 09:18:05--  https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7657 (7.5K) [text/plain]\n","Saving to: ‘TransformLayer.py.1’\n","\n","TransformLayer.py.1 100%[===================>]   7.48K  --.-KB/s    in 0s      \n","\n","2020-12-10 09:18:05 (105 MB/s) - ‘TransformLayer.py.1’ saved [7657/7657]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KztkhiXR3Zr1","executionInfo":{"status":"ok","timestamp":1607591885804,"user_tz":-60,"elapsed":4485,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":["'''\n","dataaugs:\n","https://github.com/MishaLaskin/rad/blob/1246bfd6e716669126e12c1f02f393801e1692c1/data_augs.py#L296\n","'''\n","'''\n","https://arxiv.org/pdf/2004.14990.pdf\n","'''\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from TransformLayer import ColorJitterLayer\n","\n","\n","def random_crop(imgs, out=84):\n","    \"\"\"\n","        args:\n","        imgs: np.array shape (B,C,H,W)\n","        out: output size (e.g. 84)\n","        returns np.array\n","    \"\"\"\n","    n, c, h, w = imgs.shape\n","    crop_max = h - out + 1\n","    w1 = np.random.randint(0, crop_max, n)\n","    h1 = np.random.randint(0, crop_max, n)\n","    cropped = np.empty((n, c, out, out), dtype=imgs.dtype)\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        \n","        cropped[i] = img[:, h11:h11 + out, w11:w11 + out]\n","    return cropped\n","\n","\n","def grayscale(imgs):\n","    # imgs: b x c x h x w\n","    device = imgs.device\n","    b, c, h, w = imgs.shape\n","    frames = c // 3\n","    \n","    imgs = imgs.view([b,frames,3,h,w])\n","    imgs = imgs[:, :, 0, ...] * 0.2989 + imgs[:, :, 1, ...] * 0.587 + imgs[:, :, 2, ...] * 0.114 \n","    \n","    imgs = imgs.type(torch.uint8).float()\n","    # assert len(imgs.shape) == 3, imgs.shape\n","    imgs = imgs[:, :, None, :, :]\n","    imgs = imgs * torch.ones([1, 1, 3, 1, 1], dtype=imgs.dtype).float().to(device) # broadcast tiling\n","    return imgs\n","\n","def random_grayscale(images,p=.3):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: cpu or cuda\n","        returns torch.tensor\n","    \"\"\"\n","    device = images.device\n","    in_type = images.type()\n","    images = images * 255.\n","    images = images.type(torch.uint8)\n","    # images: [B, C, H, W]\n","    bs, channels, h, w = images.shape\n","    images = images.to(device)\n","    gray_images = grayscale(images)\n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = torch.from_numpy(mask)\n","    frames = images.shape[1] // 3\n","    images = images.view(*gray_images.shape)\n","    mask = mask[:, None] * torch.ones([1, frames]).type(mask.dtype)\n","    mask = mask.type(images.dtype).to(device)\n","    mask = mask[:, :, None, None, None]\n","    out = mask * gray_images + (1 - mask) * images\n","    out = out.view([bs, -1, h, w]).type(in_type) / 255.\n","    return out\n","\n","# random cutout\n","# TODO: should mask this \n","\n","def random_cutout(imgs, min_cut=10,max_cut=30):\n","    \"\"\"\n","        args:\n","        imgs: np.array shape (B,C,H,W)\n","        min / max cut: int, min / max size of cutout \n","        returns np.array\n","    \"\"\"\n","\n","    n, c, h, w = imgs.shape\n","    w1 = np.random.randint(min_cut, max_cut, n)\n","    h1 = np.random.randint(min_cut, max_cut, n)\n","    \n","    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        cut_img = img.copy()\n","        cut_img[:, h11:h11 + h11, w11:w11 + w11] = 0\n","        #print(img[:, h11:h11 + h11, w11:w11 + w11].shape)\n","        cutouts[i] = cut_img\n","    return cutouts\n","\n","def random_cutout_color(imgs, min_cut=7,max_cut=22):\n","    \"\"\"\n","        args:\n","        imgs: shape (B,C,H,W)\n","        out: output size (e.g. 84)\n","    \"\"\"\n","    \n","    n, c, h, w = imgs.shape\n","    w1 = np.random.randint(min_cut, max_cut, n)\n","    h1 = np.random.randint(min_cut, max_cut, n)\n","    \n","    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n","    rand_box = np.random.randint(0, 255, size=(n, c)) / 255.\n","    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n","        cut_img = img.copy()\n","        \n","        # add random box\n","        cut_img[:, h11:h11 + h11, w11:w11 + w11] = np.tile(\n","            rand_box[i].reshape(-1,1,1),                                                \n","            (1,) + cut_img[:, h11:h11 + h11, w11:w11 + w11].shape[1:])\n","        \n","        cutouts[i] = cut_img\n","    return cutouts\n","\n","# random flip\n","\n","def random_flip(images,p=.2):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: cpu or gpu, \n","        p: prob of applying aug,\n","        returns torch.tensor\n","    \"\"\"\n","    # images: [B, C, H, W]\n","    device = images.device\n","    bs, channels, h, w = images.shape\n","    \n","    images = images.to(device)\n","\n","    flipped_images = images.flip([3])\n","    \n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = torch.from_numpy(mask)\n","    frames = images.shape[1] #// 3\n","    images = images.view(*flipped_images.shape)\n","    mask = mask[:, None] * torch.ones([1, frames]).type(mask.dtype)\n","    \n","    mask = mask.type(images.dtype).to(device)\n","    mask = mask[:, :, None, None]\n","    \n","    out = mask * flipped_images + (1 - mask) * images\n","\n","    out = out.view([bs, -1, h, w])\n","    return out\n","\n","# random rotation\n","\n","def random_rotation(images,p=.3):\n","    \"\"\"\n","        args:\n","        imgs: torch.tensor shape (B,C,H,W)\n","        device: str, cpu or gpu, \n","        p: float, prob of applying aug,\n","        returns torch.tensor\n","    \"\"\"\n","    device = images.device\n","    # images: [B, C, H, W]\n","    bs, channels, h, w = images.shape\n","    \n","    images = images.to(device)\n","\n","    rot90_images = images.rot90(1,[2,3])\n","    rot180_images = images.rot90(2,[2,3])\n","    rot270_images = images.rot90(3,[2,3])    \n","    \n","    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n","    rnd_rot = np.random.randint(1, 4, size=(images.shape[0],))\n","    mask = rnd <= p\n","    mask = rnd_rot * mask\n","    mask = torch.from_numpy(mask).to(device)\n","    \n","    frames = images.shape[1]\n","    masks = [torch.zeros_like(mask) for _ in range(4)]\n","    for i,m in enumerate(masks):\n","        m[torch.where(mask==i)] = 1\n","        m = m[:, None] * torch.ones([1, frames]).type(mask.dtype).type(images.dtype).to(device)\n","        m = m[:,:,None,None]\n","        masks[i] = m\n","    \n","    \n","    out = masks[0] * images + masks[1] * rot90_images + masks[2] * rot180_images + masks[3] * rot270_images\n","\n","    out = out.view([bs, -1, h, w])\n","    return out\n","\n","\n","# random color\n","\n","    \n","\n","def random_convolution(imgs):\n","    '''\n","    random covolution in \"network randomization\"\n","    \n","    (imbs): B x (C x stack) x H x W, note: imgs should be normalized and torch tensor\n","    '''\n","    _device = imgs.device\n","    \n","    img_h, img_w = imgs.shape[2], imgs.shape[3]\n","    num_stack_channel = imgs.shape[1]\n","    num_batch = imgs.shape[0]\n","    num_trans = num_batch\n","    batch_size = int(num_batch / num_trans)\n","    \n","    # initialize random covolution\n","    rand_conv = nn.Conv2d(3, 3, kernel_size=3, bias=False, padding=1).to(_device)\n","    \n","    for trans_index in range(num_trans):\n","        torch.nn.init.xavier_normal_(rand_conv.weight.data)\n","        temp_imgs = imgs[trans_index*batch_size:(trans_index+1)*batch_size]\n","        temp_imgs = temp_imgs.reshape(-1, 3, img_h, img_w) # (batch x stack, channel, h, w)\n","        rand_out = rand_conv(temp_imgs)\n","        if trans_index == 0:\n","            total_out = rand_out\n","        else:\n","            total_out = torch.cat((total_out, rand_out), 0)\n","    total_out = total_out.reshape(-1, num_stack_channel, img_h, img_w)\n","    return total_out\n","\n","\n","def random_color_jitter(imgs):\n","    \"\"\"\n","        inputs np array outputs tensor\n","    \"\"\"\n","    b,c,h,w = imgs.shape\n","    imgs = imgs.view(-1,3,h,w)\n","    transform_module = nn.Sequential(ColorJitterLayer(brightness=0.4, \n","                                                contrast=0.4,\n","                                                saturation=0.4, \n","                                                hue=0.5, \n","                                                p=1.0, \n","                                                batch_size=b,\n","                                                stack_size=1))\n","\n","    imgs = transform_module(imgs).view(b,c,h,w)\n","    return imgs\n","\n","\n","def random_translate(imgs, size, return_random_idxs=False, h1s=None, w1s=None):\n","    n, c, h, w = imgs.shape\n","    assert size >= h and size >= w\n","    outs = np.zeros((n, c, size, size), dtype=imgs.dtype)\n","    h1s = np.random.randint(0, size - h + 1, n) if h1s is None else h1s\n","    w1s = np.random.randint(0, size - w + 1, n) if w1s is None else w1s\n","    for out, img, h1, w1 in zip(outs, imgs, h1s, w1s):\n","        out[:, h1:h1 + h, w1:w1 + w] = img\n","    if return_random_idxs:  # So can do the same to another set of imgs.\n","        return outs, dict(h1s=h1s, w1s=w1s)\n","    return outs\n","\n","\n","def no_aug(x):\n","    return x\n","\n","\n","# if __name__ == '__main__':\n","#     import time \n","#     from tabulate import tabulate\n","#     def now():\n","#         return time.time()\n","#     def secs(t):\n","#         s = now() - t\n","#         tot = round((1e5 * s)/60,1)\n","#         return round(s,3),tot\n","\n","#     x = np.load('data_sample.npy',allow_pickle=True)\n","#     x = np.concatenate([x,x,x],1)\n","#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#     x = torch.from_numpy(x).to(device)\n","#     x = x.float() / 255.\n","\n","#     # crop\n","#     t = now()\n","#     random_crop(x.cpu().numpy(),64)\n","#     s1,tot1 = secs(t)\n","#     # grayscale \n","#     t = now()\n","#     random_grayscale(x,p=.5)\n","#     s2,tot2 = secs(t)\n","#     # normal cutout \n","#     t = now()\n","#     random_cutout(x.cpu().numpy(),10,30)\n","#     s3,tot3 = secs(t)\n","#     # color cutout \n","#     t = now()\n","#     random_cutout_color(x.cpu().numpy(),10,30)\n","#     s4,tot4 = secs(t)\n","#     # flip \n","#     t = now()\n","#     random_flip(x,p=.5)\n","#     s5,tot5 = secs(t)\n","#     # rotate \n","#     t = now()\n","#     random_rotation(x,p=.5)\n","#     s6,tot6 = secs(t)\n","#     # rand conv \n","#     t = now()\n","#     random_convolution(x)\n","#     s7,tot7 = secs(t)\n","#     # rand color jitter \n","#     t = now()\n","#     random_color_jitter(x)\n","#     s8,tot8 = secs(t)\n","    \n","#     print(tabulate([['Crop', s1,tot1], \n","#                     ['Grayscale', s2,tot2], \n","#                     ['Normal Cutout', s3,tot3], \n","#                     ['Color Cutout', s4,tot4], \n","#                     ['Flip', s5,tot5], \n","#                     ['Rotate', s6,tot6], \n","#                     ['Rand Conv', s7,tot7], \n","#                     ['Color Jitter', s8,tot8]], \n","#                     headers=['Data Aug', 'Time / batch (secs)', 'Time / 100k steps (mins)']))\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bn2rkllGJPtZ"},"source":["Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."]},{"cell_type":"code","metadata":{"id":"8Z8P1ehENCwc","executionInfo":{"status":"ok","timestamp":1607591886050,"user_tz":-60,"elapsed":464,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":["# Hyperparameters\n","total_steps = 10e6\n","num_envs = 32\n","num_levels = 100\n","num_steps = 256\n","num_epochs = 3\n","batch_size = 512\n","eps = .2\n","grad_eps = .5\n","value_coef = .5\n","entropy_coef = .01"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yarbuupm8Mos","executionInfo":{"status":"ok","timestamp":1607591887715,"user_tz":-60,"elapsed":1546,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","from utils import make_env, Storage, orthogonal_init\r\n","\r\n","from google.colab import files\r\n","\r\n","def xavier_uniform_init(module, gain=1.0):\r\n","    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\r\n","        nn.init.xavier_uniform_(module.weight.data, gain)\r\n","        nn.init.constant_(module.bias.data, 0)\r\n","    return module\r\n","\r\n","\r\n","\r\n","class Flatten(nn.Module):\r\n","    def forward(self, x):\r\n","        return x.view(x.size(0), -1)\r\n","\r\n","\r\n","\r\n","class ResidualBlock(nn.Module):\r\n","    def __init__(self,\r\n","                 in_channels):\r\n","        super(ResidualBlock, self).__init__()\r\n","        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\r\n","        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\r\n","\r\n","    def forward(self, x):\r\n","        out = nn.ReLU()(x)\r\n","        out = self.conv1(out)\r\n","        out = nn.ReLU()(out)\r\n","        out = self.conv2(out)\r\n","        return out + x\r\n","\r\n","class ImpalaBlock(nn.Module):\r\n","    def __init__(self, in_channels, out_channels):\r\n","        super(ImpalaBlock, self).__init__()\r\n","        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\r\n","        self.res1 = ResidualBlock(out_channels)\r\n","        self.res2 = ResidualBlock(out_channels)\r\n","\r\n","    def forward(self, x):\r\n","        x = self.conv(x)\r\n","        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\r\n","        x = self.res1(x)\r\n","        x = self.res2(x)\r\n","        return x\r\n","\r\n","class ImpalaModel(nn.Module):\r\n","    def __init__(self,\r\n","                 in_channels,\r\n","                 **kwargs):\r\n","        super(ImpalaModel, self).__init__()\r\n","        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\r\n","        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\r\n","        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\r\n","        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=256)\r\n","\r\n","        self.output_dim = 256\r\n","        self.apply(xavier_uniform_init)\r\n","\r\n","    def forward(self, x):\r\n","        x = self.block1(x)\r\n","        x = self.block2(x)\r\n","        x = self.block3(x)\r\n","        x = nn.ReLU()(x)\r\n","        x = Flatten()(x)\r\n","        x = self.fc(x)\r\n","        x = nn.ReLU()(x)\r\n","        return x\r\n","\r\n","\r\n","class Encoder(nn.Module):\r\n","  def __init__(self, in_channels, feature_dim):\r\n","    super().__init__()\r\n","    self.layers = nn.Sequential(\r\n","        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\r\n","        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\r\n","        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\r\n","        Flatten(),\r\n","        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\r\n","    )\r\n","    self.apply(orthogonal_init)\r\n","\r\n","  def forward(self, x):\r\n","    return self.layers(x)\r\n","\r\n","\r\n","class Policy(nn.Module):\r\n","  def __init__(self, encoder, feature_dim, num_actions):\r\n","    super().__init__()\r\n","    self.encoder = encoder\r\n","    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\r\n","    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\r\n","\r\n","  def act(self, x):\r\n","    with torch.no_grad():\r\n","      x = x.cuda().contiguous()\r\n","      dist, value = self.forward(x)\r\n","      action = dist.sample()\r\n","      log_prob = dist.log_prob(action)\r\n","    \r\n","    return action.cpu(), log_prob.cpu(), value.cpu()\r\n","\r\n","  def forward(self, x):\r\n","    x = self.encoder(x)\r\n","    logits = self.policy(x)\r\n","    value = self.value(x).squeeze(1)\r\n","    dist = torch.distributions.Categorical(logits=logits)\r\n","\r\n","    return dist, value\r\n","\r\n","########################################################################################################################################"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":494},"id":"yTBV9xpKpEFa","executionInfo":{"status":"error","timestamp":1607171409773,"user_tz":-60,"elapsed":124169,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}},"outputId":"09ca9b46-8365-4c3b-8bd2-3f884d625c26"},"source":["\n","\n","# Define environmentbossfight\n","# check the utils.py file for info on arguments\n","env = make_env(num_envs, num_levels=num_levels, use_backgrounds=True,env_name='starpilot')\n","print('Observation space:', env.observation_space)\n","print('Action space:', env.action_space.n)\n","\n","# Define network\n","#encoder = Encoder(3,512)\n","observation_shape = env.observation_space.shape\n","in_channels = observation_shape[0]\n","encoder = ImpalaModel(in_channels=in_channels)\n","policy = Policy(encoder, 256, 15)\n","policy.cuda()\n","# Define optimizer\n","# these are reasonable values but probably not optimal\n","optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n","\n","# Define temporary storage\n","# we use this to collect transitions during each iteration\n","storage = Storage(\n","    env.observation_space.shape,\n","    num_steps,\n","    num_envs\n",")\n","\n","\n","# Run training\n","obs = env.reset()\n","step = 0\n","i=0\n","data=[];\n","print(\"NN setup, Training Starts\")\n","while step < total_steps:\n","\n","  # Use policy to collect data for num_steps steps\n","  policy.eval()\n","  for _ in range(num_steps):\n","    # Use policy\n","    action, log_prob, value = policy.act(obs)\n","    \n","    # Take step in environment\n","    next_obs, reward, done, info = env.step(action)\n","\n","    # Store data\n","    storage.store(obs, action, reward, done, info, log_prob, value)\n","    \n","    # Update current observation\n","    obs = next_obs\n","\n","  # Add the last observation to collected data\n","  _, _, value = policy.act(obs)\n","  storage.store_last(obs, value)\n","\n","  # Compute return and advantage\n","  storage.compute_return_advantage()\n","\n","  # Optimize policy\n","  policy.train()\n","  for epoch in range(num_epochs):\n","\n","    # Iterate over batches of transitions\n","    generator = storage.get_generator(batch_size)\n","    for batch in generator:\n","      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n","\n","      'data aug'\n","      b_obs = random_cutout_color(b_obs.to('cpu').numpy())\n","      b_obs=torch.from_numpy(b_obs).to('cuda')\n","\n","      # Get current policy outputs\n","      new_dist, new_value = policy(b_obs)\n","      new_log_prob = new_dist.log_prob(b_action)\n","      # log_prob\n","      # Clipped policy objective\n","      #print(str(log_prob.shape) + \" \" + str(b_log_prob.shape) + \" \" + str(new_log_prob.shape))\n","      ratio = torch.exp(new_log_prob - b_log_prob)\n","      \n","      clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps) \n","      policy_reward = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)\n","      #clip_fraction = (abs((ratio - 1.0)) > clip).to(torch.float).mean()\n","      pi_loss = -policy_reward.mean()\n","\n","      # Clipped value function objective\n","      # clipped_value = new_value + (b_value - new_value).clamp(min=-eps,max=eps)\n","      # vf_loss=torch.max((b_value-b_returns)**2, (clipped_value-b_returns)**2)\n","      # value_loss = 0.5 * vf_loss.mean()\n","\n","      # clipped_value = b_value + (new_value - b_value).clamp(min=-eps,max=eps) \n","\n","      # clipped_value = (new_value - b_value).clamp(min=-eps,max=eps)\n","      # value_loss = 0.5 * torch.max(torch.pow(new_value - b_returns,2), torch.pow(clipped_value - b_returns, 2)).mean()\n","      value_loss = 0.5 * torch.max(torch.pow(new_value - b_returns,2), torch.pow(b_value - b_returns, 2)).mean()\n","\n","      # Entropy loss\n","      entropy_loss = new_dist.entropy().mean()\n","\n","      # Backpropagate losses\n","      # loss = torch.mean(pi_loss+value_coef*value_loss+entropy_coef*entropy_loss) #\n","      loss = pi_loss + value_coef * value_loss - entropy_coef * entropy_loss\n","      loss.backward()\n","\n","      # Clip gradients\n","      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n","\n","      # Update policy\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","  # Update stats\n","  step += num_envs * num_steps\n","  print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n","  data.append(storage.get_reward())\n","\n","  if step%1007616==0:\n","    torch.save(policy.state_dict(), 'checkpointstar'+str(i)+'.pt')\n","    i=i+1\n","\n","print('Completed training!')\n","torch.save(policy.state_dict(), 'checkpointFinalstar.pt')\n","np.save(\"data.npy\",data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n","Action space: 15\n","NN setup, Training Starts\n","Step: 8192\tMean reward: 5.21875\n","Step: 16384\tMean reward: 5.25\n","Step: 24576\tMean reward: 4.875\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-6df9843abe4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Take step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# Store data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    255\u001b[0m \t\t\"\"\"\n\u001b[1;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                         \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym3/interop.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym3/libenv.py\u001b[0m in \u001b[0;36mget_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibenv_observe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_copy_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ei-kz-dy7zUN","executionInfo":{"status":"ok","timestamp":1607592152953,"user_tz":-60,"elapsed":263691,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}}},"source":["import imageio\r\n","\r\n","# Make evaluation environment\r\n","eval_env = make_env(num_envs, env_name = 'starpilot',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\r\n","obs = eval_env.reset()\r\n","\r\n","frames = []\r\n","eval_reward_lst=[]\r\n","reward_info_lst=[]\r\n","reward_lst=[]\r\n","for i in range(10):\r\n","  # total_reward = []\r\n","  info_lst=[]\r\n","  reward_lst=[]\r\n","\r\n","  in_channels=eval_env.observation_space.shape[0]\r\n","  encoder =  ImpalaModel(in_channels=in_channels)\r\n","  policy = Policy(encoder, 256, 15)\r\n","  policy.eval()\r\n","\r\n","  if i==9:\r\n","    policy.load_state_dict(torch.load(\"checkpointFinalstar.pt\"))\r\n","  else:\r\n","    policy.load_state_dict(torch.load(f\"checkpointstar{i}.pt\"))\r\n","  \r\n","  # policy.load_state_dict(torch.load(f'checkpointbigaug{i}.pt'))\r\n","\r\n","  policy.cuda()\r\n","  reward_info_lst=[]\r\n","  total_reward=[]\r\n","  for _ in range(256):\r\n","    \r\n","    # Use policy\r\n","    action, log_prob, value = policy.act(obs)\r\n","\r\n","    # Take step in environment\r\n","    obs, reward, done, inf = eval_env.step(action)\r\n","    \r\n","    # reward_info_lst.append(info['reward'])\r\n","    # info_lst.append(info)\r\n","    total_reward.append(torch.Tensor(reward))\r\n","  \r\n","  # Calculate average return\r\n","  total_reward = torch.stack(total_reward).sum(0).mean(0)\r\n","  eval_reward_lst.append(total_reward)\r\n","  # info_lst=sum(info_lst)/len(info_lst)\r\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"GzBtQVaQ73Sl","executionInfo":{"status":"ok","timestamp":1607592153838,"user_tz":-60,"elapsed":259391,"user":{"displayName":"Haseeb Kamal","photoUrl":"","userId":"17619384286290130148"}},"outputId":"08a7f80b-af6c-43f0-b2b2-e3d040d43b52"},"source":["lstlst = [element.cpu().detach().item() for element in eval_reward_lst]\r\n","# lstlst = \r\n","print((lstlst))\r\n","\r\n","import seaborn as sns\r\n","import matplotlib.pyplot as plt\r\n","\r\n","\r\n","sns.lineplot(data=lstlst)\r\n","plt.show()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[9.5, 6.422253131866455, 6.641263961791992, 6.281245231628418, 5.8408355712890625, 5.375690460205078, 5.513914108276367, 5.848752021789551, 6.191249370574951, 5.791662216186523]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"JxRWy_T9JY4M"},"source":["Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."]},{"cell_type":"markdown","metadata":{"id":"Jp0Rfed3WFU_"},"source":["# New section"]},{"cell_type":"markdown","metadata":{"id":"RAZrWuVGLTu-"},"source":["Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."]},{"cell_type":"code","metadata":{"id":"2zecOCkd7Jzt"},"source":["# import imageio\n","# from google.colab import files\n","\n","# # Make evaluation environment\n","# eval_env = make_env(num_envs, env_name = 'starpilot', use_backgrounds=True,start_level=num_levels, num_levels=num_levels)\n","# obs = eval_env.reset()\n","\n","# frames = []\n","# total_reward = []\n","\n","# # Evaluate policy\n","# policy.eval()\n","# for _ in range(512):\n","\n","#   # Use policy\n","#   action, log_prob, value = policy.act(obs)\n","\n","#   # Take step in environment\n","#   obs, reward, done, info = eval_env.step(action)\n","#   total_reward.append(torch.Tensor(reward))\n","\n","#   # Render environment and store\n","#   frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n","#   frames.append(frame)\n","\n","# # Calculate average return\n","# total_reward = torch.stack(total_reward).sum(0).mean(0)\n","# print('Average return:', total_reward)\n","\n","# # Save frames as video\n","# frames = torch.stack(frames)\n","# imageio.mimsave('vid.mp4', frames, fps=25)\n","\n","\n","# files.download('vid.mp4')\n","\n","\n","# #Bossfight\n","# #Step: 8003584\tMean reward: 0.1875\n","# #Completed training!"],"execution_count":null,"outputs":[]}]}